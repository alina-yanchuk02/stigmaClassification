{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic classification of stigmatizing mental illness articles in online news journals - April 2022\n",
    "Author: Alina Yanchuk - alinayanchuk@ua.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents:\n",
    "\n",
    "* [3. Classification](#chapter3)\n",
    "    * [3.1 Requirements](#section_3_1)\n",
    "    * [3.2 Imports](#section_3_2)\n",
    "    * [3.3 Get data](#section_3_3)\n",
    "    * [3.4 Train and Test dataset](#section_3_4)\n",
    "    * [3.5 Generate feature vectors](#section_3_5)\n",
    "    * [3.6 Models training and optimization (Machine Learning)](#section_3_6)\n",
    "        * [3.6.1 Logistics Regression](#section_3_6_1)\n",
    "            * [3.6.1.1 With bag-of-words](#section_3_6_1_1)\n",
    "            * [3.6.1.2 With TF-IDF](#section_3_6_1_2)\n",
    "            * [3.6.1.3 With LIWC](#section_3_6_1_3)\n",
    "        * [3.6.2 Linear Support Vector Classifier (SVC)](#section_3_6_2)\n",
    "            * [3.6.2.1 With bag-of-words](#section_3_6_2_1)\n",
    "            * [3.6.2.2 With TF-IDF](#section_3_6_2_2)\n",
    "            * [3.6.2.3 With LIWC](#section_3_6_2_3)\n",
    "        * [3.6.3 Multinomial Naive Bayes](#section_3_6_3)\n",
    "            * [3.6.3.1 With bag-of-words](#section_3_6_3_1)\n",
    "            * [3.6.3.2 With TF-IDF](#section_3_6_3_2)\n",
    "            * [3.6.3.3 With LIWC](#section_3_6_3_3)\n",
    "        * [3.6.4 K-Nearest Neighbors](#section_3_6_4)\n",
    "            * [3.6.4.1 With bag-of-words](#section_3_6_4_1)\n",
    "            * [3.6.4.2 With TF-IDF](#section_3_6_4_2)\n",
    "            * [3.6.4.3 With LIWC](#section_3_6_4_3)\n",
    "        * [3.6.5 Random Forest](#section_3_6_5)\n",
    "            * [3.6.5.1 With bag-of-words](#section_3_6_5_1)\n",
    "            * [3.6.5.2 With TF-IDF](#section_3_6_5_2)\n",
    "            * [3.6.5.3 With LIWC](#section_3_6_5_3)\n",
    "    * [3.7 Models training and optimization (Deep learning)](#section_3_7)\n",
    "        * [3.7.1 Convolutional Neural Network](#section_3_7_1)\n",
    "        * [3.7.2 Long Short-Term Memory (LSTM)](#section_3_7_2)\n",
    "        * [3.7.3 Bidirectional Long Short-Term Memory (Bi-LSTM)](#section_3_7_3)\n",
    "    * [3.8 Evaluation analysis](#section_3_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "\n",
    "Text classification is a problem that belongs to the category of algorithms that\n",
    "use supervised learning and consists of an automatic process of associating data\n",
    "texts to a given class. \n",
    "\n",
    "In this step, a classification task is performed using some of the most popular Machine Learning and Deep Learning algorithms for binary text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Requirements <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydot\n",
    "#pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Imports <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 17:35:40.849836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 17:35:40.849893: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import floor, log\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as scores\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, Flatten, MaxPooling1D, LSTM, Bidirectional\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Get data <a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>prisão perpétua homem tentou assassinar senado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>john nash matemático mente brilhante morre aci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>mito reeleição mínima garantida cavaco sairá d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>morreu rita levintalcini grande dama ciência i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>trás porta amarela homem problemas psicológico...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content\n",
       "0      0  prisão perpétua homem tentou assassinar senado...\n",
       "1      0  john nash matemático mente brilhante morre aci...\n",
       "2      1  mito reeleição mínima garantida cavaco sairá d...\n",
       "3      0  morreu rita levintalcini grande dama ciência i...\n",
       "4      0  trás porta amarela homem problemas psicológico..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('data_preprocessed.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Train and Test dataset <a class=\"anchor\" id=\"section_3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news in train dataset: 782\n",
      "Number of news in test dataset: 196\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into a 80% train dataset and 20% test dataset\n",
    "\n",
    "X = data.loc[:,'content']\n",
    "y = data.loc[:,'label']\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, data.index, test_size=0.2, random_state=55)\n",
    "\n",
    "print(\"Number of news in train dataset: \" + str(len(X_train)))\n",
    "print(\"Number of news in test dataset: \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Generate feature vectors <a class=\"anchor\" id=\"section_3_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 44522)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate bag-of-words feature vectors\n",
    "\n",
    "bow_vectorizer = CountVectorizer(lowercase=False)\n",
    "bow_train = bow_vectorizer.fit_transform(X_train)\n",
    "bow_test = bow_vectorizer.transform(X_test)\n",
    "\n",
    "bow_train = (bow_train - np.min(bow_train)) / (np.max(bow_train) - np.min(bow_train)) # Normalize to range [0, 1]\n",
    "bow_test = (bow_test - np.min(bow_test)) / (np.max(bow_test) - np.min(bow_test))\n",
    "\n",
    "# (Number of news, Number of features/unique words in training dataset)\n",
    "bow_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#bow_vectorizer = pickle.load(open(\"vectors/BOW.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 44522)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TF-IDF feature vectors\n",
    "# \"words that are unique to particular document would have higher weights compared to words that are used commonly across documents\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False,)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# (Number of news, Number of features/unique words in training dataset)\n",
    "tfidf_train.shape\n",
    "\n",
    "# IF SAVED - Load saved vectorizer\n",
    "#tfidf_vectorizer = pickle.load(open(\"vectors/TFIDF.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(782, 464)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate LIWC-PT feature vectors\n",
    "# \"lexicon that assigns words to categories\"\n",
    "# http://143.107.183.175:21380/portlex/index.php/pt/projetos/liwc\n",
    "# https://lit.eecs.umich.edu/geoliwc/liwc_dictionary.html\n",
    "\n",
    "liwc_dictionary = {}\n",
    "# IF SAVED - Load saved mapping\n",
    "#liwc_dictionary = pickle.load(open(\"vectors/LIWC.pickle\", 'rb'))\n",
    "\n",
    "\n",
    "# Read the file and create the LIWC dictionary mapping (word-categories) (If not in repo, need to download from official website above)\n",
    "with open(\"dictionary/liwc.txt\", \"r\", encoding = \"ISO-8859-1\") as liwc_file:\n",
    "    start = False\n",
    "    for line in liwc_file:\n",
    "        try:\n",
    "            line = line.split()\n",
    "            if start==False:\n",
    "                if line[0]==\"a\":\n",
    "                    start=True\n",
    "                    word = line[0]\n",
    "                    vector = np.asarray(line[1:], dtype='float32')\n",
    "                    liwc_dictionary[word] = vector\n",
    "            else:\n",
    "                word = line[0]\n",
    "                vector = np.asarray(line[1:], dtype='float32')\n",
    "                liwc_dictionary[word] = vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "liwc_file.close()\n",
    "\n",
    "# Create the feature vectors: each vector will have all the LIWC categories and the corresponding values will be incremented everytime a word in the news article (document) fits in that category\n",
    "\n",
    "documents_train = list(X_train.values.flatten()) # List of all news\n",
    "documents_train = [string.split() for string in documents_train] # Split each news article in tokens/words\n",
    "\n",
    "liwc_train = np.zeros((len(X_train), 464))\n",
    "for document in documents_train:\n",
    "    i = documents_train.index(document)\n",
    "    for word in document:\n",
    "        if word in liwc_dictionary:\n",
    "            vector = liwc_dictionary[word]\n",
    "            for index in vector:\n",
    "                liwc_train[i][int(index)-1] = liwc_train[i][int(index)-1] + 1\n",
    "\n",
    "documents_test = list(X_test.values.flatten())\n",
    "documents_test = [string.split() for string in documents_test]\n",
    "\n",
    "liwc_test = np.zeros((len(X_test), 464))\n",
    "for document in documents_test:\n",
    "    i = documents_test.index(document)\n",
    "    for word in document:\n",
    "        if word in liwc_dictionary:\n",
    "            vector = liwc_dictionary[word]\n",
    "            for index in vector:\n",
    "                liwc_test[i][int(index)-1] = liwc_test[i][int(index)-1] + 1\n",
    "\n",
    "liwc_train = (liwc_train - np.min(liwc_train)) / (np.max(liwc_train) - np.min(liwc_train)) # Normalize to range [0, 1]\n",
    "liwc_test = (liwc_test - np.min(liwc_test)) / (np.max(liwc_test) - np.min(liwc_test)) \n",
    "\n",
    "# (Number of news, Number of features/categories in training dataset)\n",
    "liwc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51285, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Word Embeddings vectors and matrix\n",
    "# Glove PT 300D from NILC-Embeddings \n",
    "# http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
    "\n",
    "words_per_article = list(map(lambda x : len(x.split()),data.content))\n",
    "avg = sum(words_per_article)/len(words_per_article) # Average number of words per news article\n",
    "avg = pow(2, floor(log(avg)/log(2))) # Convert to nearest power of 2\n",
    "\n",
    "results = Counter()\n",
    "data['content'].str.lower().str.split().apply(results.update) # Number of unique words in all dataset\n",
    "\n",
    "max_length_content = int(avg) # Max number of content that each object/news article will have after padding\n",
    "vocabulary_length = len(results) # Max number of unique words\n",
    "\n",
    "# Create word index\n",
    "token = text.Tokenizer(lower=False, num_words=vocabulary_length) \n",
    "token.fit_on_texts(data['content']) # Tokenize all corpus\n",
    "word_index = token.word_index # Index of unique words (dictionary)\n",
    "\n",
    "# Convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "we_train = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_length_content)\n",
    "we_test = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_length_content)\n",
    "\n",
    "# Load the pre-trained word-embedding vectors (Glove 300D)\n",
    "embedding_index = {}\n",
    "# IF SAVED - Load saved mapping\n",
    "#embedding_index = pickle.load(open(\"vectors/WE_glove.pickle\", 'rb'))\n",
    "\n",
    "# If not in repo, need to download from official website above\n",
    "with open(\"pre-trained/glove_s300.txt\", \"r\") as we_file:\n",
    "    first_line = True\n",
    "    for line in we_file:\n",
    "        try:\n",
    "            if first_line: # Ignore header\n",
    "                first_line = False\n",
    "            else:\n",
    "                line = line.split()\n",
    "                word = line[0]\n",
    "                vector = np.asarray(line[1:], dtype='float32')\n",
    "                embedding_index[word] = vector\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Create token-embedding mapping (with words from our dataset)\n",
    "embedding_matrix = np.zeros((vocabulary_length, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i > vocabulary_length - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#(Number (vocabulary size) of features/unique words in all dataset, Number of dimensions/features)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('vectors/TFIDF.pickle', 'wb') as f1:\\n    pickle.dump(tfidf_vectorizer, f1)\\nf1.close()\\n\\nwith open('vectors/BOW.pickle', 'wb') as f2:\\n    pickle.dump(bow_vectorizer, f2)\\nf2.close()\\n\\nwith open('vectors/WE.pickle', 'wb') as f3:\\n    pickle.dump(embedding_index, f3)\\nf3.close()\\n\\nwith open('vectors/LIWC.pickle', 'wb') as f4:\\n    pickle.dump(liwc_dictionary, f4)\\nf4.close()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save feature vectorizers and mappings\n",
    "\n",
    "\n",
    "with open('vectors/TFIDF.pickle', 'wb') as f1:\n",
    "    pickle.dump(tfidf_vectorizer, f1)\n",
    "f1.close()\n",
    "\n",
    "with open('vectors/BOW.pickle', 'wb') as f2:\n",
    "    pickle.dump(bow_vectorizer, f2)\n",
    "f2.close()\n",
    "\n",
    "with open('vectors/WE.pickle', 'wb') as f3:\n",
    "    pickle.dump(embedding_index, f3)\n",
    "f3.close()\n",
    "\n",
    "with open('vectors/LIWC.pickle', 'wb') as f4:\n",
    "    pickle.dump(liwc_dictionary, f4)\n",
    "f4.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Models training and optimization (Machine Learning) <a class=\"anchor\" id=\"section_3_6\"></a>\n",
    "\n",
    "#### Training\n",
    "\n",
    "Attempt to get the best combination of weights and bias to the model to minimize the loss function over the prediction range and incrementally improve model's ability to predict the classes.\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. They define how our model is actually structured. Hyper-parameter tuning is just an optimization loop on top of ML model learning to find the set of hyper-parameters leading to the lowest error on the validation set.\n",
    "\n",
    "-  The optimization for the classic Machine Learning algorithms is performed with the K-fold cross (K=5) validation strategy, in order to essentially combine training and validation data for both learning the model parameters and evaluating the model without introducing data leakage.\n",
    "\n",
    "    Library used -> Scikit-optimize: uses a Sequential model-based optimization algorithm to find optimal solutions for hyperparameter search problems in less time. \n",
    "    \n",
    "    Reference: https://scikit-optimize.github.io/stable/auto_examples/hyperparameter-optimization.html\n",
    "\n",
    "- Optimization for the Deep Learning algorithms:\n",
    "\n",
    "    Library used -> Keras Tuner: The Hyperband algorithm is a variation of random search, but with some explore-exploit theory to find the best time allocation for each of the configurations.\n",
    "    \n",
    "    Reference: https://www.tensorflow.org/tutorials/keras/keras_tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store evaluation metrics for all models\n",
    "\n",
    "evaluation_metrics = []\n",
    "\n",
    "# Function to store evaluation metrics for a model\n",
    "\n",
    "def evaluation(evaluation_metrics_list, model, model_name, test_features, test_labels, feature_vectorizer_name, deep_learning=False):\n",
    "\n",
    "    y_pred = model.predict(test_features)\n",
    "\n",
    "    if deep_learning:\n",
    "        y_pred = [int(round(p[0])) for p in y_pred]\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = accuracy_score(test_labels, y_pred)*100\n",
    "\n",
    "    # Precision, recall, f1 scores\n",
    "    precision, recall, f1score, support = scores(y_test, y_pred, average='binary')\n",
    "\n",
    "    # Add metrics to evaluation list\n",
    "    evaluation_metrics_list.append(dict([\n",
    "        ('Model', model_name),\n",
    "        ('Feature Vectorizer', feature_vectorizer_name),\n",
    "        ('Accuracy (%)', round(accuracy, 2)),\n",
    "        ('Precision', round(precision, 3)),\n",
    "        ('Recall', round(recall, 3)),\n",
    "        ('F1', round(f1score, 3))\n",
    "    ]))\n",
    "\n",
    "    return evaluation_metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict labels (using the trained model) to a dataset\n",
    "\n",
    "def prediction(trained_model, features):\n",
    "\n",
    "    y_pred = trained_model.predict(features)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert predicted labels to original data's csv file\n",
    "\n",
    "def insert(pred_labels, file_name):\n",
    "\n",
    "    original_data = pd.read_pickle(file_name)\n",
    "\n",
    "    original_data.insert(9, 'Predicted label', pred_labels)\n",
    "\n",
    "    original_data.to_csv(\"result_\" + file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Logistic Regression <a class=\"anchor\" id=\"section_2_6_1\"></a>\n",
    "\n",
    "- Is a classical linear method for binary classification (fits a line to best separate the two classes);\n",
    "- It can handle both dense and sparse input;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=9.367025\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0) # solver: algorithm to use in the optimization problem (liblinear for small datasets)\n",
    "#IF SAVED - Load saved model\n",
    "#model_lr_bow = pickle.load(open(\"models/LogisticRegressionBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_bow = LogisticRegression(random_state=0, C=best_C_lr_bow, solver=\"liblinear\").fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_bow, model_name=\"Logistic Regression\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Logistic Regression (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=3.843817\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_lr_tfidf = pickle.load(open(\"models/LogisticRegressionTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_tfidf = LogisticRegression(random_state=0, C=best_C_lr_tfidf, solver=\"liblinear\").fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_tfidf, model_name=\"Logistic Regression\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Logistic Regression (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1.3 With LIWC <a class=\"anchor\" id=\"section_3_6_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=9.694427\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_lr_liwc = LogisticRegression(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_lr_LIWC = pickle.load(open(\"models/LogisticRegressionLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_lr_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_lr_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_lr_liwc = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_lr_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (LIWC) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_lr_liwc = LogisticRegression(random_state=0, C=best_C_lr_liwc, solver=\"liblinear\").fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LogisticRegressionLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_lr_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_lr_liwc, model_name=\"Logistic Regression\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Logistic Regression (LIWC) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Linear Support Vector Classifier (SVC) <a class=\"anchor\" id=\"section_3_6_2\"></a>\n",
    "\n",
    " - Supports both dense and sparse input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=12.819720\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_bow = pickle.load(open(\"models/LinearSVCBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_bow = LinearSVC(random_state=0, C=best_C_svc_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_bow, model_name=\"Support Vector Classifier (Linear)\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Linear Suppport Vector Classifier (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/alina/anaconda3/envs/stigma/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=9.299825\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_tfidf = pickle.load(open(\"models/LinearSVCTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_tfidf = LinearSVC(random_state=0, C=best_C_svc_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_tfidf, model_name=\"Support Vector Classifier (Linear)\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Linear Support Vector Classifier (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2.4 With LIWC <a class=\"anchor\" id=\"section_3_6_2_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - C=11.332275\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_svc_liwc = LinearSVC(random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_svc_liwc = pickle.load(open(\"models/LinearSVCLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(1, 15, name='C')] # Inverse of regularization strength\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_svc_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_svc_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_C_svc_liwc = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - C=%f\"\"\" % (best_C_svc_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Classifier (LIWC) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_svc_liwc = LinearSVC(random_state=0, C=best_C_svc_liwc).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LinearSVCLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_svc_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_svc_liwc, model_name=\"Support Vector Classifier (Linear)\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Linear Support Vector Classifier (LIWC) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3 Multinomial Naive Bayes <a class=\"anchor\" id=\"section_3_6_3\"></a>\n",
    "\n",
    "- Probabilistic approach to classifying documents in the case of acknowledging the frequency of a specified word in a text document;\n",
    "- Achieves well on discrete types as the number of words found in a document;\n",
    "- Conditional independence is assumed in real data and it attempts to approximate to the optimal soltuion;\n",
    "- Is a quick classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_3_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - alpha=0.001813\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_nb_bow = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_bow = pickle.load(open(\"models/MultinomialNaiveBayesBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_bow = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_bow = MultinomialNB(alpha=best_alpha_nb_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_bow, model_name=\"Naive Bayes (Multinomial)\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_3_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - alpha=0.338005\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_nb_tfidf = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_tfidf = pickle.load(open(\"models/MultinomialNaiveBayesTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_tfidf = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_tfidf = MultinomialNB(alpha=best_alpha_nb_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_tfidf, model_name=\"Naive Bayes (Multinomial)\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (TF-IDF) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3.3 With LIWC <a class=\"anchor\" id=\"section_3_6_3_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - alpha=0.000200\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "\n",
    "model_nb_liwc = MultinomialNB()\n",
    "# IF SAVED - Load saved model\n",
    "#model_nb_liwc = pickle.load(open(\"models/MultinomialNaiveBayesLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Real(0, 10, name='alpha')] # Additive (Laplace/Lidstone) smoothing parameter\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_nb_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_nb_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_alpha_nb_liwc = res_gp.x[0]\n",
    "\n",
    "print(\"\"\"Best parameters: - alpha=%f\"\"\" % (best_alpha_nb_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (LIWC) sucessfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_nb_liwc = MultinomialNB(alpha=best_alpha_nb_liwc).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/MultinomialNaiveBayesLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_nb_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_nb_liwc, model_name=\"Naive Bayes (Multinomial)\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes (LIWC) sucessfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.4 K-Nearest Neighbors <a class=\"anchor\" id=\"section_3_6_4\"></a>\n",
    "\n",
    "- The data is classified based on vote among the k nearest neighbors;\n",
    "- Number of neighbors can be sqrt(number of data objects) = int(sqrt(978)) = 31;\n",
    "- Should be preferred when the data-set is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - N_neighbors=1 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier() \n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_bow = pickle.load(open(\"models/KNNBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 35, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_bow = res_gp.x[0]\n",
    "best_metric_knn_bow = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_bow, best_metric_knn_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_bow = KNeighborsClassifier(n_neighbors=best_n_knn_bow, metric=best_metric_knn_bow).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_bow, model_name=\"K-Nearest Neighbors\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - N_neighbors=33 Metric= euclidean\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier()\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_tfidf = pickle.load(open(\"models/KNNTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 35, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_tfidf = res_gp.x[0]\n",
    "best_metric_knn_tfidf = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_tfidf, best_metric_knn_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_tfidf = KNeighborsClassifier(n_neighbors=best_n_knn_tfidf, metric=best_metric_knn_tfidf).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_tfidf, model_name=\"K-Nearest Neighbors\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4.2 With LIWC <a class=\"anchor\" id=\"section_3_6_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - N_neighbors=16 Metric= manhattan\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_knn_liwc = KNeighborsClassifier()\n",
    "# IF SAVED - Load saved model\n",
    "#model_knn_liwc = pickle.load(open(\"models/KNNLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(1, 35, name='n_neighbors'), # Number of neighbors to use\n",
    "          Categorical([\"euclidean\", \"manhattan\"], name='metric')] # The distance metric to use for the tree\n",
    "            \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_knn_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_knn_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_n_knn_liwc = res_gp.x[0]\n",
    "best_metric_knn_liwc = res_gp.x[1]\n",
    "\n",
    "print(\"Best parameters: - N_neighbors=%d Metric= %s\" % (best_n_knn_liwc, best_metric_knn_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (LIWC) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_knn_liwc = KNeighborsClassifier(n_neighbors=best_n_knn_liwc, metric=best_metric_knn_liwc).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/KNNLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_knn_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_knn_liwc, model_name=\"K-Nearest Neighbors\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"K-Nearest Neighbors (LIWC) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.5 Random Forest <a class=\"anchor\" id=\"section_3_6_5\"></a>\n",
    "\n",
    "- Fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5.1 With bag-of-words <a class=\"anchor\" id=\"section_3_6_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - n_estimators=128 max_depth=40\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_bow = pickle.load(open(\"models/RandomForestBOW.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(10, 200, name='n_estimators'), # the number of trees in the forest\n",
    "          Integer(3,100, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_bow.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_bow, bow_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_bow = res_gp.x[0]\n",
    "best_maxdepth_rf_bow = res_gp.x[1]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth=%d\"\"\" % (best_nestimators_rf_bow, best_maxdepth_rf_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Bag-of-words) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_bow = RandomForestClassifier(n_estimators=best_nestimators_rf_bow, max_depth=best_maxdepth_rf_bow, criterion='entropy', random_state=0).fit(bow_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestBOW.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_bow, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_bow, model_name=\"Random Forest\", test_features=bow_test, test_labels=y_test, feature_vectorizer_name=\"Bag-of-words\")\n",
    "\n",
    "print(\"Random Forest (Bag-of-words) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5.2 With TF-IDF <a class=\"anchor\" id=\"section_3_6_5_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - n_estimators=100 max_depth:25\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_tfidf = pickle.load(open(\"models/RandomForestTFIDF.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(100, 300, name='n_estimators'),\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_tfidf.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_tfidf, tfidf_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_tfidf = res_gp.x[0]\n",
    "best_maxdepth_rf_tfidf = res_gp.x[1]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth:%d\"\"\" % (best_nestimators_rf_tfidf, best_maxdepth_rf_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (TF-IDF) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_tfidf = RandomForestClassifier(n_estimators=best_nestimators_rf_tfidf, max_depth=best_maxdepth_rf_tfidf, criterion='entropy', random_state=0).fit(tfidf_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestTFIDF.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_tfidf, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_tfidf, model_name=\"Random Forest\", test_features=tfidf_test, test_labels=y_test, feature_vectorizer_name=\"TF-IDF\")\n",
    "\n",
    "print(\"Random Forest (TF-IDF) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.5.3 With LIWC <a class=\"anchor\" id=\"section_3_6_5_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: - n_estimators=181 max_depth:26\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "model_rf_liwc = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\n",
    "# IF SAVED - Load saved model\n",
    "#model_rf_liwc = pickle.load(open(\"models/RandomForestLIWC.pickle\", 'rb'))\n",
    "\n",
    "# Hyper-parameteres tuning\n",
    "\n",
    "space  = [Integer(100, 300, name='n_estimators'),\n",
    "          Integer(3,30, name ='max_depth')] # the maximum allowable depth for each decision tree \n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    model_rf_liwc.set_params(**params)\n",
    "\n",
    "    return -np.mean(cross_val_score(model_rf_liwc, liwc_train, y_train, cv=5, n_jobs=-1, scoring=\"neg_mean_absolute_error\"))\n",
    "\n",
    "res_gp = gp_minimize(objective, space, random_state=0)\n",
    "\n",
    "best_nestimators_rf_liwc = res_gp.x[0]\n",
    "best_maxdepth_rf_liwc = res_gp.x[1]\n",
    "\n",
    "print(\"\"\"Best parameters: - n_estimators=%d max_depth:%d\"\"\" % (best_nestimators_rf_liwc, best_maxdepth_rf_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (LIWC) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "model_rf_liwc = RandomForestClassifier(n_estimators=best_nestimators_rf_liwc, max_depth=best_maxdepth_rf_liwc, criterion='entropy', random_state=0).fit(liwc_train, y_train)\n",
    "\n",
    "# Model saving\n",
    "with open('models/RandomForestLIWC.pickle', 'wb') as f:\n",
    "    pickle.dump(model_rf_liwc, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=model_rf_liwc, model_name=\"Random Forest\", test_features=liwc_test, test_labels=y_test, feature_vectorizer_name=\"LIWC\")\n",
    "\n",
    "print(\"Random Forest (LIWC) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Models training and optimization (Deep Learning) <a class=\"anchor\" id=\"section_3_7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Convolutional Neural Network <a class=\"anchor\" id=\"section_3_7_1\"></a>\n",
    "\n",
    "- Multi-layered artificial neural networks with the ability to detect complex features in data;\n",
    "- Have been showing good results on text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def create_cnn_model_for_tuning(hp):\n",
    "    model_cnn_we = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model_cnn_we.add(Embedding(vocabulary_length, 300, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "    hp_rates = hp.Choice('rate', values=[0.2,0.3,0.4,0.5])\n",
    "    model_cnn_we.add(Dropout(rate=hp_rates))\n",
    "    hp_filters = hp.Int('filters', min_value = 10, max_value = 60)\n",
    "    hp_kernels = hp.Int('kernel_size', min_value = 3, max_value = 15)\n",
    "    model_cnn_we.add(Conv1D(filters=hp_filters, kernel_size=hp_kernels, padding=\"valid\", activation=\"relu\"))\n",
    "    model_cnn_we.add(MaxPooling1D())\n",
    "    model_cnn_we.add(Conv1D(filters=hp_filters, kernel_size=hp_kernels, padding=\"valid\", activation=\"relu\"))\n",
    "    model_cnn_we.add(MaxPooling1D())\n",
    "    model_cnn_we.add(Flatten())\n",
    "    hp_units = hp.Int('units', min_value = 10, max_value = 100, step=10)\n",
    "    model_cnn_we.add(Dense(units=hp_units, activation=\"relu\"))\n",
    "    model_cnn_we.add(Dropout(rate=hp_rates))\n",
    "    model_cnn_we.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "    model_cnn_we.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    return model_cnn_we\n",
    "\n",
    "# IF SAVED - Load saved model\n",
    "#model_cnn_we = pickle.load(open(\"models/CNNWE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "tuner = kt.Hyperband(create_cnn_model_for_tuning, objective='val_accuracy', max_epochs=5, factor=3, directory='hp_tuning', project_name='cnn')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(we_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hp=tuner.get_best_hyperparameters()[0]\n",
    "best_model_cnn_we = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model_cnn_we.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 4s 130ms/step - loss: 0.6778 - accuracy: 0.5921\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.3602 - accuracy: 0.8645\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.2316 - accuracy: 0.9092\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.1669 - accuracy: 0.9488\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.1439 - accuracy: 0.9514\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0989 - accuracy: 0.9680\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0710 - accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.1118 - accuracy: 0.9514\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0716 - accuracy: 0.9744\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0279 - accuracy: 0.9949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 15:50:20.093890: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e63c8ee-aa02-4183-af84-c4246c059496/assets\n",
      "CNN (Word Embeddings) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "best_model_cnn_we.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "#plot_model(model_cnn_we, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Model saving\n",
    "with open('models/CNNWE.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_cnn_we, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=best_model_cnn_we, model_name=\"Convolutional Neural Network\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"CNN (Word Embeddings) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Long Short-Term memory (LSTM)<a class=\"anchor\" id=\"section_3_7_2\"></a>\n",
    "\n",
    "- Type of recurrent neural network that is better in terms of memory;\n",
    "- Holds the required information and discard the information which is not required or useful for further prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def create_lstm_model_for_tuning(hp):\n",
    "    model_lstm_we = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model_lstm_we.add(Embedding(vocabulary_length, 300, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "    model_lstm_we.add(LSTM(units=hp.Int('units',min_value=32, max_value=512, step=32), activation=\"sigmoid\"))\n",
    "    model_lstm_we.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "    model_lstm_we.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model_lstm_we\n",
    "\n",
    "# IF SAVED - Load saved model\n",
    "#model_lstm_we = pickle.load(open(\"models/LSTMWE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 09m 48s]\n",
      "val_accuracy: 0.7643312215805054\n",
      "\n",
      "Best val_accuracy So Far: 0.808917224407196\n",
      "Total elapsed time: 00h 17m 04s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 512, 300)          15385500  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 224)               470400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 225       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,856,125\n",
      "Trainable params: 470,625\n",
      "Non-trainable params: 15,385,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "tuner = kt.Hyperband(create_lstm_model_for_tuning, objective='val_accuracy', max_epochs=5, factor=3, directory='hp_tuning', project_name='lstm')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(we_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hp=tuner.get_best_hyperparameters()[0]\n",
    "best_model_lstm_we = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model_lstm_we.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 46s 2s/step - loss: 0.6547 - accuracy: 0.6138\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.5729 - accuracy: 0.6982\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.5373 - accuracy: 0.7327\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.4395 - accuracy: 0.8107\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.3895 - accuracy: 0.8389\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.3202 - accuracy: 0.8645\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 43s 2s/step - loss: 0.2609 - accuracy: 0.8939\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 41s 2s/step - loss: 0.2183 - accuracy: 0.9156\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 37s 1s/step - loss: 0.2969 - accuracy: 0.8657\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 44s 2s/step - loss: 0.2377 - accuracy: 0.8990\n",
      "INFO:tensorflow:Assets written to: ram://8a43f0c2-c72a-4ad3-85e3-c7b8252f99c9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f29d4079ac0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM (Word Embeddings) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "best_model_lstm_we.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# Model saving\n",
    "with open('models/LSTMWE.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_lstm_we, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=best_model_lstm_we, model_name=\"Long short-term memory (LSTM)\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"LSTM (Word Embeddings) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3 Bidirectional Long short-term memory (Bi-LSTM) <a class=\"anchor\" id=\"section_3_7_3\"></a>\n",
    "\n",
    "- Bidirection: propagates the input forward and backwards through the RNN layer and then concatenates the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "def create_bilstm_model_for_tuning(hp):\n",
    "    model_bilstm_we = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model_bilstm_we.add(Embedding(vocabulary_length, 300, weights=[embedding_matrix], trainable=False, input_length=max_length_content))\n",
    "    model_bilstm_we.add(Bidirectional(LSTM(units=hp.Int('units',min_value=32, max_value=512, step=32), activation=\"sigmoid\")))\n",
    "    model_bilstm_we.add(Dense(1, activation=\"sigmoid\")) # binary classification\n",
    "\n",
    "    model_bilstm_we.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model_bilstm_we\n",
    "\n",
    "# IF SAVED - Load saved model\n",
    "#model_bilstm_we = pickle.load(open(\"models/BILSTMWE.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 18m 09s]\n",
      "val_accuracy: 0.7324841022491455\n",
      "\n",
      "Best val_accuracy So Far: 0.7770700454711914\n",
      "Total elapsed time: 01h 17m 41s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 512, 300)          15385500  \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 640)              1589760   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 641       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,975,901\n",
      "Trainable params: 1,590,401\n",
      "Non-trainable params: 15,385,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameteres tuning\n",
    "\n",
    "tuner = kt.Hyperband(create_bilstm_model_for_tuning, objective='val_accuracy', max_epochs=5, factor=3, directory='hp_tuning', project_name='bilstm')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(we_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hp=tuner.get_best_hyperparameters()[0]\n",
    "best_model_bilstm_we = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "best_model_bilstm_we.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 135s 5s/step - loss: 0.6830 - accuracy: 0.5563\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 130s 5s/step - loss: 0.5739 - accuracy: 0.7072\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 130s 5s/step - loss: 0.4904 - accuracy: 0.7596\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 131s 5s/step - loss: 0.3949 - accuracy: 0.8120\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 132s 5s/step - loss: 0.3606 - accuracy: 0.8325\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 133s 5s/step - loss: 0.3025 - accuracy: 0.8772\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 132s 5s/step - loss: 0.2701 - accuracy: 0.8951\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 133s 5s/step - loss: 0.2280 - accuracy: 0.9143\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 133s 5s/step - loss: 0.2859 - accuracy: 0.8900\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 135s 5s/step - loss: 0.1703 - accuracy: 0.9335\n",
      "INFO:tensorflow:Assets written to: ram://055ba164-c3c2-484d-b32b-c701834ecff3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://055ba164-c3c2-484d-b32b-c701834ecff3/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f29e7908670> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f29e7908a60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-LSTM (Word Embeddings) successfully trained and stored.\n"
     ]
    }
   ],
   "source": [
    "# Model training with BEST hyper-parameteres\n",
    "\n",
    "best_model_bilstm_we.fit(we_train, y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# Model saving\n",
    "with open('models/BILSTMWE.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_bilstm_we, f)\n",
    "\n",
    "# Store evaluation metrics\n",
    "evaluation_metrics = evaluation(evaluation_metrics_list=evaluation_metrics, model=best_model_bilstm_we, model_name=\"Bidirectional Long short-term memory (Bi-LSTM)\", test_features=we_test, test_labels=y_test, feature_vectorizer_name=\"Word Embeddings\", deep_learning=True)\n",
    "\n",
    "print(\"Bi-LSTM (Word Embeddings) successfully trained and stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Evaluation analysis <a class=\"anchor\" id=\"section_3_8\"></a>\n",
    "\n",
    "The final evaluation (with the test dataset) is performed in the optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes (Multinomial)</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>93.37</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>93.37</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Bag-of-words</td>\n",
       "      <td>92.86</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag-of-words</td>\n",
       "      <td>92.35</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>91.84</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes (Multinomial)</td>\n",
       "      <td>Bag-of-words</td>\n",
       "      <td>91.33</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bidirectional Long short-term memory (Bi-LSTM)</td>\n",
       "      <td>Word Embeddings</td>\n",
       "      <td>91.33</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>91.33</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Support Vector Classifier (Linear)</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>90.82</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Classifier (Linear)</td>\n",
       "      <td>Bag-of-words</td>\n",
       "      <td>90.31</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "      <td>Word Embeddings</td>\n",
       "      <td>87.76</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Long short-term memory (LSTM)</td>\n",
       "      <td>Word Embeddings</td>\n",
       "      <td>87.24</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Support Vector Classifier (Linear)</td>\n",
       "      <td>LIWC</td>\n",
       "      <td>80.10</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>LIWC</td>\n",
       "      <td>79.08</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>LIWC</td>\n",
       "      <td>70.41</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>LIWC</td>\n",
       "      <td>70.41</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>Bag-of-words</td>\n",
       "      <td>65.82</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes (Multinomial)</td>\n",
       "      <td>LIWC</td>\n",
       "      <td>52.04</td>\n",
       "      <td>0.520</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model Feature Vectorizer  \\\n",
       "10                       Naive Bayes (Multinomial)             TF-IDF   \n",
       "4                              Logistic Regression             TF-IDF   \n",
       "15                                   Random Forest       Bag-of-words   \n",
       "3                              Logistic Regression       Bag-of-words   \n",
       "16                                   Random Forest             TF-IDF   \n",
       "9                        Naive Bayes (Multinomial)       Bag-of-words   \n",
       "2   Bidirectional Long short-term memory (Bi-LSTM)    Word Embeddings   \n",
       "13                             K-Nearest Neighbors             TF-IDF   \n",
       "7               Support Vector Classifier (Linear)             TF-IDF   \n",
       "6               Support Vector Classifier (Linear)       Bag-of-words   \n",
       "0                     Convolutional Neural Network    Word Embeddings   \n",
       "1                    Long short-term memory (LSTM)    Word Embeddings   \n",
       "8               Support Vector Classifier (Linear)               LIWC   \n",
       "17                                   Random Forest               LIWC   \n",
       "14                             K-Nearest Neighbors               LIWC   \n",
       "5                              Logistic Regression               LIWC   \n",
       "12                             K-Nearest Neighbors       Bag-of-words   \n",
       "11                       Naive Bayes (Multinomial)               LIWC   \n",
       "\n",
       "    Accuracy  Precision  Recall     F1  \n",
       "10     93.37      0.908   0.971  0.938  \n",
       "4      93.37      0.932   0.941  0.937  \n",
       "15     92.86      0.900   0.971  0.934  \n",
       "3      92.35      0.922   0.931  0.927  \n",
       "16     91.84      0.884   0.971  0.925  \n",
       "9      91.33      0.913   0.922  0.917  \n",
       "2      91.33      0.897   0.941  0.919  \n",
       "13     91.33      0.921   0.912  0.916  \n",
       "7      90.82      0.929   0.892  0.910  \n",
       "6      90.31      0.919   0.892  0.905  \n",
       "0      87.76      0.924   0.833  0.876  \n",
       "1      87.24      0.964   0.784  0.865  \n",
       "8      80.10      0.812   0.804  0.808  \n",
       "17     79.08      0.765   0.863  0.811  \n",
       "14     70.41      0.724   0.696  0.710  \n",
       "5      70.41      0.729   0.686  0.707  \n",
       "12     65.82      0.889   0.392  0.544  \n",
       "11     52.04      0.520   1.000  0.685  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation = pd.DataFrame(data=evaluation_metrics)\n",
    "evaluation.columns = ['Model', 'Feature Vectorizer', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "evaluation = evaluation.sort_values(by='Accuracy', ascending=False)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
