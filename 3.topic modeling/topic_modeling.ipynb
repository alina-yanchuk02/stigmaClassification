{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic classification of stigmatizing mental illness articles in online news journals - April 2022\n",
    "Author: Alina Yanchuk - alinayanchuk@ua.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents:\n",
    "\n",
    "* [4. Topic modeling](#chapter4)\n",
    "    * [4.1 Requirements](#section_4_1)\n",
    "    * [4.2 Imports](#section_4_2)\n",
    "    * [4.3 Get data](#section_4_3)\n",
    "    * [4.4 With top2vec](#section_4_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Topic modeling <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a machine learning technique (unsupervised) that automatically analyzes text data to determine cluster words (mapped to topics) for a set of documents.\n",
    "\n",
    "Top2Vec is an algorithm for topic modeling and semantic search. It automatically detects topics present in text and generates jointly embedded topic, document and word vectors. Some benefits: automatically finds number of topics, works on short text, doesn't ingore semantics.\n",
    "\n",
    "Note: execute with GPU.\n",
    "\n",
    "References: 1.https://github.com/ddangelov/Top2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Requirements <a class=\"anchor\" id=\"section_4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install top2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Imports <a class=\"anchor\" id=\"section_4_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 17:24:53.067600: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-18 17:24:53.067726: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Get data <a class=\"anchor\" id=\"section_4_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>prisão perpétua homem tentou assassinar senado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>john nash matemático mente brilhante morre aci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>mito reeleição mínima garantida cavaco sairá d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>morreu rita levintalcini grande dama ciência i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>trás porta amarela homem problemas psicológico...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label                                            content\n",
       "0   0      0  prisão perpétua homem tentou assassinar senado...\n",
       "1   1      0  john nash matemático mente brilhante morre aci...\n",
       "2   2      1  mito reeleição mínima garantida cavaco sairá d...\n",
       "3   3      0  morreu rita levintalcini grande dama ciência i...\n",
       "4   4      0  trás porta amarela homem problemas psicológico..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top2vec doesn't need pre-processing, but we will still use the already cleaned dataset\n",
    "\n",
    "data = pd.read_pickle('data_preprocessed_tm.pkl')\n",
    "data.insert(0, 'ID', range(0, len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping index-document ID\n",
    "\n",
    "ids = {}\n",
    "for index in data.index:\n",
    "  ids[index] = data.iloc[index].ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    prisão perpétua homem tentou assassinar senado...\n",
       "1    john nash matemático mente brilhante morre aci...\n",
       "2    mito reeleição mínima garantida cavaco sairá d...\n",
       "3    morreu rita levintalcini grande dama ciência i...\n",
       "4    trás porta amarela homem problemas psicológico...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = data.loc[:,'content']\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 With top2vec <a class=\"anchor\" id=\"section_4_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to list of strings\n",
    "\n",
    "documents = list(content.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Top2Vec model on our news dataset\n",
    "\n",
    "model = Top2Vec(documents, speed=\"learn\", workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 10 topics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of topics found\n",
    "\n",
    "total_topics = model.get_num_topics()\n",
    "\n",
    "print(\"Found: \"+str(total_topics)+\" topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, the top 50 words are returned, in order of semantic similarity to the topic\n",
    "\n",
    "topic_words, word_scores, topic_nums = model.get_topics(total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "array([['doencas', 'estudo', 'doenca', 'medicamentos', 'ansiedade',\n",
      "        'sintomas', 'doentes', 'estudos', 'saude', 'tratamentos',\n",
      "        'tratamento', 'mental', 'mentais', 'pacientes', 'investigadores',\n",
      "        'existem', 'efeitos', 'utilizacao', 'genetica', 'comportamentos',\n",
      "        'medica', 'secundarios', 'risco', 'sofrem', 'substancia',\n",
      "        'aumentar', 'depressao', 'psicoticos', 'psiquiatria',\n",
      "        'investigador', 'destes', 'perturbacoes', 'associado', 'tipos',\n",
      "        'esquizofrenia', 'cerebro', 'casos', 'graves', 'psiquiatricos',\n",
      "        'uso', 'medico', 'medicacao', 'perda', 'alucinacoes',\n",
      "        'cientistas', 'desenvolver', 'psiquiatra', 'tratar', 'cuidados',\n",
      "        'consumo'],\n",
      "       ['homicidio', 'prisao', 'policia', 'crime', 'encontrado',\n",
      "        'crimes', 'inimputavel', 'tribunal', 'matou', 'sofre',\n",
      "        'psiquiatrica', 'vitima', 'arguido', 'psiquiatrico',\n",
      "        'internamento', 'internado', 'matar', 'acusacao', 'acusado',\n",
      "        'condenado', 'suspeito', 'julgamento', 'mata', 'filho',\n",
      "        'hospital', 'apos', 'judiciaria', 'perturbacao', 'arguida',\n",
      "        'carro', 'morte', 'juiz', 'arma', 'breivik', 'autoridades',\n",
      "        'psiquiatras', 'pena', 'individuo', 'chao', 'homem', 'morto',\n",
      "        'pediu', 'psicotico', 'mae', 'violencia', 'penal', 'jovem',\n",
      "        'psicose', 'data', 'tiro'],\n",
      "       ['filme', 'comedia', 'realizador', 'personagens', 'cinema',\n",
      "        'personagem', 'actores', 'filmes', 'original', 'estreia',\n",
      "        'actor', 'hollywood', 'serie', 'americano', 'cena', 'peca',\n",
      "        'titulo', 'oscar', 'temporada', 'obra', 'olhos', 'historia',\n",
      "        'festival', 'escrita', 'james', 'narrativa', 'imagens', 'rosto',\n",
      "        'americana', 'universo', 'the', 'teatro', 'michael', 'ficcao',\n",
      "        'artista', 'john', 'talento', 'mente', 'epoca', 'artistas',\n",
      "        'arte', 'carreira', 'televisao', 'fotografia', 'musical',\n",
      "        'romance', 'genero', 'realizado', 'sente', 'tom'],\n",
      "       ['europeia', 'austeridade', 'divida', 'euro', 'mercados',\n",
      "        'orcamental', 'uniao', 'europeu', 'economica', 'economia',\n",
      "        'economico', 'investimento', 'financas', 'europeias', 'bruxelas',\n",
      "        'defice', 'crescimento', 'crise', 'europa', 'cidadaos',\n",
      "        'financeira', 'governo', 'medidas', 'governos', 'nacionais',\n",
      "        'reduzir', 'paises', 'precisamos', 'orcamento', 'europeus',\n",
      "        'portugal', 'impostos', 'eurogrupo', 'fmi', 'publicas',\n",
      "        'reformas', 'politicas', 'mercado', 'propostas', 'governacao',\n",
      "        'politicos', 'politica', 'solucao', 'grecia', 'publica',\n",
      "        'estrategia', 'essencial', 'feira', 'centeno', 'democracia'],\n",
      "       ['eua', 'russia', 'militar', 'armas', 'washington', 'forcas',\n",
      "        'americanos', 'nortemericana', 'guerra', 'militares', 'ataque',\n",
      "        'seguranca', 'conflito', 'putin', 'nortemericano', 'norte',\n",
      "        'ataques', 'estrangeiros', 'estados', 'presidente', 'israel',\n",
      "        'americano', 'autoridades', 'administracao', 'capital', 'branca',\n",
      "        'paises', 'arafat', 'unidos', 'mortos', 'franca', 'cidades',\n",
      "        'humanos', 'contra', 'arma', 'chefe', 'trump', 'global',\n",
      "        'sentimento', 'semana', 'regiao', 'populacao', 'eleicoes',\n",
      "        'povo', 'america', 'sob', 'milhares', 'estado', 'ontem',\n",
      "        'presidencia'],\n",
      "       ['partido', 'governo', 'psd', 'parlamentar', 'mocao',\n",
      "        'parlamento', 'politico', 'socialista', 'cds', 'lider', 'coelho',\n",
      "        'partidos', 'oposicao', 'pcp', 'passos', 'socialistas',\n",
      "        'politica', 'socrates', 'eleitoral', 'voto', 'esquerda',\n",
      "        'eleicoes', 'republica', 'governacao', 'debate', 'votos',\n",
      "        'presidente', 'campanha', 'mandato', 'deputado', 'bloco',\n",
      "        'assembleia', 'jose', 'candidato', 'censura', 'politicas',\n",
      "        'posicao', 'confianca', 'sousa', 'democratica', 'discurso',\n",
      "        'creio', 'costa', 'iniciativa', 'cavaco', 'executivo',\n",
      "        'comissao', 'declaracoes', 'divida', 'estrategia'],\n",
      "       ['livros', 'escritor', 'literatura', 'escritores', 'escrita',\n",
      "        'escrever', 'romance', 'obra', 'escreve', 'livro', 'textos',\n",
      "        'escrevi', 'ler', 'escrito', 'personagens', 'nasceu', 'leitores',\n",
      "        'autor', 'paginas', 'irmao', 'escreveu', 'morrer', 'historias',\n",
      "        'percebi', 'historia', 'epoca', 'cartas', 'escrevo', 'linguagem',\n",
      "        'pai', 'fiquei', 'mulher', 'irmaos', 'mim', 'pensei',\n",
      "        'jornalismo', 'longa', 'vida', 'gostava', 'comecei', 'robert',\n",
      "        'talvez', 'personagem', 'guerra', 'editora', 'ficcao', 'critica',\n",
      "        'jornais', 'contar', 'talento'],\n",
      "       ['banda', 'album', 'disco', 'pop', 'rock', 'musica', 'cancoes',\n",
      "        'musical', 'concerto', 'concertos', 'cancao', 'musico', 'bandas',\n",
      "        'palco', 'cantar', 'letras', 'editora', 'som', 'the', 'estreia',\n",
      "        'tocar', 'vivo', 'and', 'festival', 'temas', 'primeiro',\n",
      "        'artistas', 'ouvir', 'minutos', 'voz', 'regresso', 'surgiu',\n",
      "        'nomes', 'sensacao', 'tiago', 'surge', 'natural', 'tudo',\n",
      "        'terceiro', 'primeira', 'sinto', 'uns', 'tema', 'marca',\n",
      "        'comecei', 'festa', 'elementos', 'pouco', 'amor', 'registo'],\n",
      "       ['desporto', 'futebol', 'jogo', 'lideranca', 'dirigentes',\n",
      "        'jogos', 'valores', 'vitoria', 'clube', 'rio', 'liga', 'equipa',\n",
      "        'exercicio', 'etica', 'paixao', 'proprios', 'porto', 'gestao',\n",
      "        'caracteristicas', 'estilo', 'fim', 'poder', 'vantagem',\n",
      "        'espanhol', 'proxima', 'pontos', 'funcoes', 'soares', 'logica',\n",
      "        'daqueles', 'alias', 'busca', 'luta', 'certo', 'clara', 'rui',\n",
      "        'especie', 'humana', 'interesses', 'oportunidade', 'estrategia',\n",
      "        'cena', 'espirito', 'democratica', 'partido', 'bem', 'costa',\n",
      "        'sul', 'simples', 'passagem'],\n",
      "       ['magistrados', 'justica', 'judicial', 'tribunais', 'ministerio',\n",
      "        'penal', 'processos', 'criminal', 'juizes', 'advogados', 'elina',\n",
      "        'fraga', 'corrupcao', 'gestao', 'cidadao', 'direito',\n",
      "        'politicos', 'codigo', 'judiciaria', 'segredo', 'meios',\n",
      "        'independencia', 'processo', 'funcionarios', 'tribunal',\n",
      "        'decisao', 'sistema', 'conselho', 'interesses', 'publico',\n",
      "        'reformas', 'ordem', 'estado', 'caso', 'lei', 'republica',\n",
      "        'leis', 'impostos', 'superior', 'democratica', 'alias',\n",
      "        'investigacao', 'combate', 'autoridade', 'governo', 'poder',\n",
      "        'abertura', 'accao', 'agentes', 'reforma']], dtype='<U15')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 50 most relevant words for each topic\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "array([[0.66990924, 0.63975966, 0.62038016, 0.61935025, 0.617918  ,\n",
      "        0.6161783 , 0.6157184 , 0.6069458 , 0.59013665, 0.5872875 ,\n",
      "        0.57665765, 0.57321715, 0.5730446 , 0.5637206 , 0.5499758 ,\n",
      "        0.5452902 , 0.5446592 , 0.5391974 , 0.539122  , 0.53110605,\n",
      "        0.5299418 , 0.5295639 , 0.5276976 , 0.5265435 , 0.52526313,\n",
      "        0.5237713 , 0.5234479 , 0.52059764, 0.51766413, 0.5146994 ,\n",
      "        0.506858  , 0.5032734 , 0.502999  , 0.50271577, 0.5025605 ,\n",
      "        0.50228304, 0.4988374 , 0.4969626 , 0.49252406, 0.4922303 ,\n",
      "        0.4845404 , 0.4840532 , 0.48088893, 0.47902697, 0.4772811 ,\n",
      "        0.4766344 , 0.47609985, 0.4746457 , 0.47357175, 0.47322702],\n",
      "       [0.63625103, 0.62617075, 0.6212035 , 0.618063  , 0.5796766 ,\n",
      "        0.57817936, 0.5777791 , 0.5777204 , 0.564726  , 0.5645703 ,\n",
      "        0.56097203, 0.5569245 , 0.54993206, 0.5369404 , 0.5229149 ,\n",
      "        0.5223444 , 0.51159775, 0.51016814, 0.508513  , 0.5083684 ,\n",
      "        0.5010875 , 0.48722336, 0.47801363, 0.46606117, 0.4652979 ,\n",
      "        0.46441987, 0.46354172, 0.46154132, 0.45925328, 0.4463732 ,\n",
      "        0.4429441 , 0.44250053, 0.44203138, 0.43588656, 0.43357405,\n",
      "        0.43089405, 0.42688018, 0.4206208 , 0.42004794, 0.41922218,\n",
      "        0.4174756 , 0.4145272 , 0.41417766, 0.413521  , 0.4128879 ,\n",
      "        0.41184232, 0.410718  , 0.41029167, 0.41006058, 0.40936166],\n",
      "       [0.71461904, 0.6846367 , 0.67789644, 0.66589326, 0.65143603,\n",
      "        0.6410696 , 0.62945926, 0.62618446, 0.5999335 , 0.5808935 ,\n",
      "        0.57826525, 0.5593025 , 0.5181955 , 0.49177748, 0.482784  ,\n",
      "        0.47527742, 0.47185326, 0.45830107, 0.44432816, 0.4441051 ,\n",
      "        0.4428185 , 0.4420191 , 0.43663737, 0.43621922, 0.43610498,\n",
      "        0.4356858 , 0.43380725, 0.43327034, 0.42712525, 0.42709866,\n",
      "        0.42479163, 0.41534832, 0.41531536, 0.41455525, 0.4096731 ,\n",
      "        0.40820226, 0.4078159 , 0.40575776, 0.40555915, 0.40275466,\n",
      "        0.39357576, 0.3932134 , 0.3879034 , 0.3854023 , 0.38151965,\n",
      "        0.37938726, 0.37724   , 0.37587237, 0.37228552, 0.37046638],\n",
      "       [0.66440356, 0.6550735 , 0.63556904, 0.6232313 , 0.62256575,\n",
      "        0.6211982 , 0.60466564, 0.5905872 , 0.5858978 , 0.5854816 ,\n",
      "        0.5747193 , 0.5733186 , 0.54542905, 0.5452089 , 0.53830296,\n",
      "        0.52800864, 0.5243011 , 0.51845247, 0.5147099 , 0.5097731 ,\n",
      "        0.5058155 , 0.4915632 , 0.48965612, 0.48151702, 0.4761983 ,\n",
      "        0.47618493, 0.47086626, 0.4622998 , 0.46157545, 0.4562642 ,\n",
      "        0.45431852, 0.45075303, 0.44249815, 0.4399729 , 0.43779188,\n",
      "        0.43579033, 0.4355861 , 0.43469492, 0.4309993 , 0.42825168,\n",
      "        0.4215707 , 0.4213985 , 0.42030075, 0.41995206, 0.40625045,\n",
      "        0.40425318, 0.40301287, 0.40256232, 0.40157583, 0.40109414],\n",
      "       [0.5533638 , 0.55013764, 0.54563886, 0.51682746, 0.50463367,\n",
      "        0.4917032 , 0.49044374, 0.4747519 , 0.47407788, 0.46604863,\n",
      "        0.46140236, 0.4612685 , 0.45974424, 0.45651326, 0.44571394,\n",
      "        0.44246107, 0.4361366 , 0.42825136, 0.42598248, 0.4256326 ,\n",
      "        0.4199248 , 0.41308942, 0.41244218, 0.4117226 , 0.39852524,\n",
      "        0.3975878 , 0.38754702, 0.3872424 , 0.3870357 , 0.3861949 ,\n",
      "        0.38600317, 0.3825444 , 0.3781142 , 0.3752717 , 0.3745296 ,\n",
      "        0.37390396, 0.37069792, 0.36767036, 0.36683142, 0.36230633,\n",
      "        0.36159235, 0.35864976, 0.35712907, 0.35695222, 0.3554614 ,\n",
      "        0.35543805, 0.3551893 , 0.3477884 , 0.3467618 , 0.34639493],\n",
      "       [0.67779326, 0.6719321 , 0.6711121 , 0.64341724, 0.62424666,\n",
      "        0.5981312 , 0.5884691 , 0.5856968 , 0.58025205, 0.5791276 ,\n",
      "        0.57793933, 0.5767175 , 0.5764765 , 0.5513367 , 0.55056655,\n",
      "        0.5441006 , 0.53889406, 0.5380576 , 0.5345878 , 0.52985865,\n",
      "        0.5265834 , 0.5236602 , 0.52315265, 0.5161426 , 0.50621927,\n",
      "        0.5004466 , 0.49531487, 0.49286965, 0.48521563, 0.48197362,\n",
      "        0.47162575, 0.47131798, 0.47096077, 0.46316618, 0.45857984,\n",
      "        0.45215887, 0.45051298, 0.4498781 , 0.43834093, 0.42729348,\n",
      "        0.42672378, 0.42453444, 0.42190436, 0.41975057, 0.41912398,\n",
      "        0.4069264 , 0.4054921 , 0.40395835, 0.40230238, 0.40128   ],\n",
      "       [0.65266424, 0.6487285 , 0.61957943, 0.61650836, 0.6151754 ,\n",
      "        0.5803834 , 0.57471156, 0.5393437 , 0.5359083 , 0.52996665,\n",
      "        0.5266061 , 0.4880919 , 0.48317125, 0.47877857, 0.4770909 ,\n",
      "        0.47264087, 0.47051877, 0.46665743, 0.46275902, 0.4545193 ,\n",
      "        0.44487485, 0.43880492, 0.43590537, 0.4330396 , 0.43070364,\n",
      "        0.42680508, 0.42357948, 0.4172809 , 0.41691643, 0.40850857,\n",
      "        0.40764993, 0.4070311 , 0.40391284, 0.39661065, 0.39592153,\n",
      "        0.39234036, 0.38369668, 0.38350436, 0.38330802, 0.3832959 ,\n",
      "        0.38066822, 0.37984294, 0.3778826 , 0.37654454, 0.37615505,\n",
      "        0.37522218, 0.373374  , 0.3662608 , 0.36409974, 0.36407033],\n",
      "       [0.7988832 , 0.7783157 , 0.771822  , 0.7493992 , 0.74535143,\n",
      "        0.7378021 , 0.7348221 , 0.7165348 , 0.6904269 , 0.6795033 ,\n",
      "        0.67868316, 0.67286545, 0.638116  , 0.6207878 , 0.6183935 ,\n",
      "        0.5633106 , 0.5631187 , 0.56275797, 0.52717966, 0.5148629 ,\n",
      "        0.51385224, 0.487281  , 0.46023935, 0.45941702, 0.4421373 ,\n",
      "        0.43126306, 0.42359084, 0.41953415, 0.4185282 , 0.41720432,\n",
      "        0.4119148 , 0.404467  , 0.4000377 , 0.38700178, 0.38102365,\n",
      "        0.37991664, 0.37952682, 0.37289098, 0.3709355 , 0.36808297,\n",
      "        0.36495802, 0.35972148, 0.3585876 , 0.35808697, 0.3558727 ,\n",
      "        0.34434602, 0.33783174, 0.3342662 , 0.3341104 , 0.33357823],\n",
      "       [0.56468356, 0.5637729 , 0.5495653 , 0.5003802 , 0.46772844,\n",
      "        0.46079928, 0.41492173, 0.40070543, 0.39946607, 0.38808584,\n",
      "        0.3867941 , 0.37838134, 0.36593837, 0.3639279 , 0.36041704,\n",
      "        0.3563515 , 0.35550433, 0.34751195, 0.32846278, 0.3282487 ,\n",
      "        0.32728803, 0.32681692, 0.32205373, 0.31914   , 0.31297764,\n",
      "        0.30625513, 0.30295086, 0.30224392, 0.30167207, 0.30005056,\n",
      "        0.29725966, 0.2972362 , 0.29718137, 0.29698232, 0.2957277 ,\n",
      "        0.29390997, 0.292503  , 0.29181182, 0.28979614, 0.28975704,\n",
      "        0.2895775 , 0.28943297, 0.28871563, 0.28863442, 0.28620347,\n",
      "        0.2841203 , 0.2839477 , 0.28227794, 0.2821736 , 0.28130904],\n",
      "       [0.77059567, 0.6968638 , 0.66810745, 0.64551574, 0.6387552 ,\n",
      "        0.60010177, 0.58431035, 0.57648385, 0.57525533, 0.5669233 ,\n",
      "        0.5236878 , 0.52343416, 0.51172817, 0.50037956, 0.49543947,\n",
      "        0.4954386 , 0.49334326, 0.48072222, 0.4803483 , 0.47823557,\n",
      "        0.4646963 , 0.46234262, 0.46215603, 0.45912617, 0.45429257,\n",
      "        0.43458286, 0.4245436 , 0.42396268, 0.4227118 , 0.4151058 ,\n",
      "        0.40874293, 0.40595484, 0.40335116, 0.3975455 , 0.39178556,\n",
      "        0.38888696, 0.38553482, 0.3835213 , 0.38219026, 0.37399578,\n",
      "        0.36559352, 0.36222365, 0.35664967, 0.35615593, 0.35360846,\n",
      "        0.35184687, 0.34816897, 0.34760666, 0.34599292, 0.3423584 ]],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# And their scores\n",
    "\n",
    "word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for each topic\n",
    "\n",
    "for topic in topic_nums:\n",
    "    model.generate_topic_wordcloud(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0 has 232 documents.\n",
      "Topic 1 has 158 documents.\n",
      "Topic 2 has 112 documents.\n",
      "Topic 3 has 92 documents.\n",
      "Topic 4 has 85 documents.\n",
      "Topic 5 has 80 documents.\n",
      "Topic 6 has 70 documents.\n",
      "Topic 7 has 70 documents.\n",
      "Topic 8 has 41 documents.\n",
      "Topic 9 has 38 documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of documents in each topic\n",
    "\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "for i in topic_nums:\n",
    "  print(\"Topic \"+str(i)+\" has \"+str(topic_sizes[i])+\" documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search documents by topic. Ordered by (decreasing) similarity.\n",
    "# Note: in every execution of this notebook, the topics retrieved may be slightly different. Adapt this part to your results.\n",
    "\n",
    "# Topic 0\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=0, num_docs=topic_sizes[0])\n",
    "documents_topic0 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic0.append(ids.get(index))\n",
    "\n",
    "# Topic 1\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=1, num_docs=topic_sizes[1])\n",
    "documents_topic1 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic1.append(ids.get(index))\n",
    "\n",
    "documents_topic1\n",
    "\n",
    "# Topic 2\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=2, num_docs=topic_sizes[2])\n",
    "documents_topic2 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic2.append(ids.get(index))\n",
    "\n",
    "# Topic 3\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=3, num_docs=topic_sizes[3])\n",
    "documents_topic3 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic3.append(ids.get(index))\n",
    "\n",
    "# Topic 4\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=4, num_docs=topic_sizes[4])\n",
    "documents_topic4 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic4.append(ids.get(index))\n",
    "\n",
    "# Topic 5\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=5, num_docs=topic_sizes[5])\n",
    "documents_topic5 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic5.append(ids.get(index))\n",
    "\n",
    "# Topic 6\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=6, num_docs=topic_sizes[6])\n",
    "documents_topic6 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic6.append(ids.get(index))\n",
    "\n",
    "# Topic 7\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=7, num_docs=topic_sizes[7])\n",
    "documents_topic7 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic7.append(ids.get(index))\n",
    "\n",
    "# Topic 8\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=8, num_docs=topic_sizes[8])\n",
    "documents_topic8 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic8.append(ids.get(index))\n",
    "\n",
    "# Topic 9\n",
    "documents, document_scores, document_indexes = model.search_documents_by_topic(topic_num=9, num_docs=topic_sizes[9])\n",
    "documents_topic9 = []\n",
    "for index in document_indexes:\n",
    "  documents_topic9.append(ids.get(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>journal</th>\n",
       "      <th>journalTitle</th>\n",
       "      <th>content</th>\n",
       "      <th>authors</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>archiveDate</th>\n",
       "      <th>year</th>\n",
       "      <th>linkToArchive</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>literal</td>\n",
       "      <td>publico.pt</td>\n",
       "      <td>Público</td>\n",
       "      <td>dia janeiro jared loughner tentou matar sucess...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-12-30</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://arquivo.pt/wayback/20121230181331/http...</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>literal</td>\n",
       "      <td>publico.pt</td>\n",
       "      <td>Público</td>\n",
       "      <td>john nash matemático nobel economia retratado ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://arquivo.pt/wayback/20160117223452/http...</td>\n",
       "      <td>Cinema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>estigmatizante</td>\n",
       "      <td>publico.pt</td>\n",
       "      <td>Público</td>\n",
       "      <td>cavaco sairá desta campanha pior entrou casos ...</td>\n",
       "      <td>['Nuno Ferreira Santos', 'Arquivo']</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-01-21</td>\n",
       "      <td>2011</td>\n",
       "      <td>https://arquivo.pt/wayback/20110121142608/http...</td>\n",
       "      <td>Política</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>literal</td>\n",
       "      <td>publico.pt</td>\n",
       "      <td>Público</td>\n",
       "      <td>cientista senadora italiana rita levintalcini ...</td>\n",
       "      <td>['Clara Barata']</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-01-17</td>\n",
       "      <td>2013</td>\n",
       "      <td>https://arquivo.pt/wayback/20130117170513/http...</td>\n",
       "      <td>Conflitos Militares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>literal</td>\n",
       "      <td>publico.pt</td>\n",
       "      <td>Público</td>\n",
       "      <td>ninguém sabe fazer ninguém sabe pensa come sob...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-04-20</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://arquivo.pt/wayback/20150420143056/http...</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID           label     journal journalTitle  \\\n",
       "0   0         literal  publico.pt      Público   \n",
       "1   1         literal  publico.pt      Público   \n",
       "2   2  estigmatizante  publico.pt      Público   \n",
       "3   3         literal  publico.pt      Público   \n",
       "4   4         literal  publico.pt      Público   \n",
       "\n",
       "                                             content  \\\n",
       "0  dia janeiro jared loughner tentou matar sucess...   \n",
       "1  john nash matemático nobel economia retratado ...   \n",
       "2  cavaco sairá desta campanha pior entrou casos ...   \n",
       "3  cientista senadora italiana rita levintalcini ...   \n",
       "4  ninguém sabe fazer ninguém sabe pensa come sob...   \n",
       "\n",
       "                               authors publishDate archiveDate  year  \\\n",
       "0                                   []        None  2012-12-30  2012   \n",
       "1                                   []        None  2016-01-17  2016   \n",
       "2  ['Nuno Ferreira Santos', 'Arquivo']        None  2011-01-21  2011   \n",
       "3                     ['Clara Barata']        None  2013-01-17  2013   \n",
       "4                                   []        None  2015-04-20  2015   \n",
       "\n",
       "                                       linkToArchive                topic  \n",
       "0  https://arquivo.pt/wayback/20121230181331/http...                Crime  \n",
       "1  https://arquivo.pt/wayback/20160117223452/http...               Cinema  \n",
       "2  https://arquivo.pt/wayback/20110121142608/http...             Política  \n",
       "3  https://arquivo.pt/wayback/20130117170513/http...  Conflitos Militares  \n",
       "4  https://arquivo.pt/wayback/20150420143056/http...                Crime  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add topics to Visualization and Analysis file \n",
    "# Note: in every execution of this notebook, the topics retrieved may be slightly different. Adapt this part to your results.\n",
    "\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "\n",
    "data_va = pd.read_pickle(parent+'/4.visualization and analysis/data_preprocessed_va.pkl')\n",
    "data_va.head()\n",
    "\n",
    "data_va.insert(10, 'topic', \"\")\n",
    "\n",
    "def add_topics(id):\n",
    "\n",
    "    topic = \" \"\n",
    "     \n",
    "    if id in documents_topic0:\n",
    "      topic = \"Saúde\"\n",
    "    elif id in documents_topic1:\n",
    "      topic = \"Crime\"\n",
    "    elif id in documents_topic2:\n",
    "      topic = \"Cinema\"\n",
    "    elif id in documents_topic3:\n",
    "      topic = \"Economia\"\n",
    "    elif id in documents_topic4:\n",
    "      topic = \"Conflitos militares\"\n",
    "    elif id in documents_topic5:\n",
    "      topic = \"Política\"\n",
    "    elif id in documents_topic6:\n",
    "      topic = \"Literatura\"\n",
    "    elif id in documents_topic7:\n",
    "      topic = \"Música\"\n",
    "    elif id in documents_topic8:\n",
    "      topic = \"Desporto\"\n",
    "    elif id in documents_topic9:\n",
    "      topic = \"Justiça\"\n",
    "\n",
    "    return topic\n",
    "    \n",
    "data_va[\"topic\"] = data_va.ID.apply(lambda x: add_topics(x))\n",
    "data_va.head()\n",
    "\n",
    "data_va.to_pickle(parent+\"/4.visualization and analysis/data_preprocessed_va.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('stigma': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4bb6a5a8d9de86797f248650fa8544a019a00d157c3095d36faee5c402c003b5"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
